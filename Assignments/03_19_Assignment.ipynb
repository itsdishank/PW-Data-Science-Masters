{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "66e8f8bf-c2ac-4673-8342-076bb492ebc3",
   "metadata": {},
   "source": [
    "### Q1. What is Min-Max scaling, and how is it used in data preprocessing? Provide an example to illustrate its application."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "944a57fc-b992-4bd4-85b2-665c48bf7113",
   "metadata": {},
   "source": [
    "Min-Max scaling is a data preprocessing technique that scales numeric features to a specific range (usually 0 to 1). It involves subtracting the minimum value and dividing by the range between the maximum and minimum values. This ensures that all values are proportionally mapped to the desired range.\n",
    "\n",
    "Example: Let's say we have a dataset of house areas ranging from 800 to 2000 square feet. To apply Min-Max scaling, we subtract the minimum value (800) from each value and divide by the range (2000-800). For a house with an area of 1200 square feet, the scaled value would be (1200-800)/(2000-800) = 0.333. This process is repeated for all data points, resulting in normalized values within the range 0 to 1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48cd96ff-6b84-4ad6-af30-af87bbb420f7",
   "metadata": {},
   "source": [
    "### Q2. What is the Unit Vector technique in feature scaling, and how does it differ from Min-Max scaling? Provide an example to illustrate its application."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d97b8c6b-92d8-4365-81a8-791ff5cfa256",
   "metadata": {},
   "source": [
    "\n",
    "The Unit Vector technique scales feature values to have a unit norm (length of 1) by dividing each data point by the Euclidean norm of the feature vector. It emphasizes the direction of the data points rather than their magnitudes. In contrast, Min-Max scaling rescales values to a specific range (e.g., 0 to 1) by subtracting the minimum value and dividing by the range.\n",
    "\n",
    "Example: Consider a dataset with two features, \"height\" (ranging from 150 to 190 cm) and \"weight\" (ranging from 50 to 80 kg). Unit Vector scaling divides each data point by the Euclidean norm of the feature vector. For a person with height 170 cm and weight 60 kg, the \"height\" value becomes 170/sqrt(170^2 + 60^2) and the \"weight\" value becomes 60/sqrt(170^2 + 60^2), ensuring a unit norm. Min-Max scaling, however, rescales the values to a specific range."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0259715d-74c2-47e1-bbfc-ba4f2fd7f0b2",
   "metadata": {},
   "source": [
    "### Q3. What is PCA (Principle Component Analysis), and how is it used in dimensionality reduction? Provide an example to illustrate its application."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "881da0d4-ffe3-4cf8-91e7-386220a89ccd",
   "metadata": {},
   "source": [
    "PCA (Principal Component Analysis) is a dimensionality reduction technique used to transform high-dimensional data into a lower-dimensional space while preserving the most important information. It identifies the principal components, which are linear combinations of the original features that capture the maximum variance in the data.\n",
    "\n",
    "Here's an example to illustrate the application of PCA:\n",
    "\n",
    "Suppose we have a dataset of houses with features such as area, number of bedrooms, and price. We want to reduce the dimensionality of the dataset using PCA.\n",
    "\n",
    "First, PCA computes the covariance matrix of the data and finds the eigenvectors (principal components) associated with the largest eigenvalues. These eigenvectors represent the directions of maximum variance in the data.\n",
    "\n",
    "Let's say we find two principal components: PC1 and PC2. We can then project the data onto these components, creating a lower-dimensional representation. Each data point is transformed into a new coordinate system defined by PC1 and PC2.\n",
    "\n",
    "By selecting a subset of the principal components, we can reduce the dimensionality of the data while retaining most of the important information. This can be useful for visualization, data compression, or improving computational efficiency in machine learning tasks.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "959f7dae-5080-4b76-977d-578350c26248",
   "metadata": {},
   "source": [
    "### Q4. What is the relationship between PCA and Feature Extraction, and how can PCA be used for Feature Extraction? Provide an example to illustrate this concept."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bef712fe-96c1-4c93-b3bd-f00fff1720b0",
   "metadata": {},
   "source": [
    "PCA can be used as a feature extraction technique in addition to its role in dimensionality reduction. In feature extraction, the goal is to derive a set of new features (or combinations of existing features) that capture the most relevant information in the data.\n",
    "\n",
    "Here's an example to illustrate the use of PCA for feature extraction:\n",
    "\n",
    "Suppose we have a dataset of images represented by pixel intensities. Each image has thousands of features (pixels), making it computationally expensive for machine learning algorithms. We want to extract a smaller set of features that still capture the important information.\n",
    "\n",
    "By applying PCA to the image dataset, we can identify the principal components (eigenvectors) that explain the maximum variance in the images. These principal components represent the most salient patterns or structures in the images.\n",
    "\n",
    "We can then select a subset of the principal components as the new features. These new features are linear combinations of the original pixel intensities and capture the essential information in a more compact form. The dimensionality of the feature space is reduced while still preserving the important characteristics of the images.\n",
    "\n",
    "Using the extracted features, we can train machine learning models more efficiently and effectively, as the model can focus on the most relevant information captured by the principal components. Feature extraction with PCA can also help mitigate the curse of dimensionality and improve generalization performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09740096-36b0-46a0-b835-85e0a5c093db",
   "metadata": {},
   "source": [
    "### Q5. You are working on a project to build a recommendation system for a food delivery service. The dataset contains features such as price, rating, and delivery time. Explain how you would use Min-Max scaling to preprocess the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9092c1d1-9fc5-43b1-8231-1b6a63ea1857",
   "metadata": {},
   "source": [
    "To preprocess the data for building a recommendation system for a food delivery service, you can use Min-Max scaling on the numerical features like price, rating, and delivery time. Here's how you would apply Min-Max scaling:\n",
    "\n",
    "1. Identify the numerical features in the dataset, such as price, rating, and delivery time.\n",
    "2. Determine the minimum and maximum values for each feature in the dataset.\n",
    "3. Subtract the minimum value from each data point and divide it by the range (maximum minus minimum) for that feature.\n",
    "4. Apply the Min-Max scaling formula.\n",
    "5. Repeat the above steps for each numerical feature in the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4daad6f-87c2-49d9-9440-10928f010ed8",
   "metadata": {},
   "source": [
    "### Q6. You are working on a project to build a model to predict stock prices. The dataset contains many features, such as company financial data and market trends. Explain how you would use PCA to reduce the dimensionality of the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06f3faeb-70f4-4614-ba8f-8e740018e21c",
   "metadata": {},
   "source": [
    "can use PCA (Principal Component Analysis). Here's how you would use PCA for dimensionality reduction:\n",
    "\n",
    "* Identify the features in the dataset, including company financial data and market trends, that you want to use for stock price prediction.\n",
    "* Standardize the features by subtracting the mean and scaling to unit variance. This step ensures that features with different scales do not dominate the PCA process.\n",
    "* Apply PCA to the standardized feature matrix. PCA calculates the principal components, which are linear combinations of the original features that capture the maximum variance in the data.\n",
    "* Determine the number of principal components to retain based on a variance threshold or the desired dimensionality reduction.\n",
    "* Select the top principal components that explain the most variance in the data. These components represent the most important patterns or structures in the feature space.\n",
    "* Use the selected principal components as the reduced feature set for the stock price prediction model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d55c316-c4b7-4ad3-adca-4e05c4afbad3",
   "metadata": {},
   "source": [
    "### Q7. For a dataset containing the following values: [1, 5, 10, 15, 20], perform Min-Max scaling to transform the values to a range of -1 to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7c756f1d-d43e-4f31-aa64-e348844ce3c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1.         -0.57894737 -0.05263158  0.47368421  1.        ]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import minmax_scale\n",
    "\n",
    "data = [1, 5, 10, 15, 20]\n",
    "scaled_data = minmax_scale(data, feature_range=(-1, 1))\n",
    "print(scaled_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faa17d79-da93-43d3-aab3-fa952ca4e018",
   "metadata": {},
   "source": [
    "### Q8. For a dataset containing the following features: [height, weight, age, gender, blood pressure], perform Feature Extraction using PCA. How many principal components would you choose to retain, and why?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d328671-c37a-4c91-a7b7-98fdd0e1bf7a",
   "metadata": {},
   "source": [
    "To perform feature extraction using PCA on the given dataset with features [height, weight, age, gender, blood pressure], the number of principal components to retain depends on the desired trade-off between dimensionality reduction and preserving information.\n",
    "\n",
    "Here's a general approach to decide on the number of principal components to retain:\n",
    "\n",
    "1. Standardize the dataset: Before applying PCA, it is advisable to standardize the features to have zero mean and unit variance to prevent features with larger scales from dominating the PCA process.\n",
    "2. Compute the covariance matrix: Calculate the covariance matrix of the standardized dataset. The covariance matrix provides insights into the relationships and variances between the features.\n",
    "3. Perform PCA: Apply PCA to the covariance matrix or the standardized dataset. PCA will identify the principal components (eigenvectors) and their associated eigenvalues. The eigenvalues represent the amount of variance explained by each principal component.\n",
    "4. Analyze the explained variance: Examine the explained variance ratio or cumulative explained variance to understand how much information is captured by each principal component. This can help decide on the number of components to retain.\n",
    "5. Choose the number of principal components: Select the number of principal components that explain a significant amount of the variance in the dataset. A commonly used rule of thumb is to retain principal components that cumulatively explain around 80% to 90% of the variance. However, the choice ultimately depends on the specific requirements of the application."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a4a3091-7977-435b-9950-e21c695fc2bb",
   "metadata": {},
   "source": [
    "### "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "668bcd92-0817-4eee-8ff2-752b9929254c",
   "metadata": {},
   "source": [
    "### "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0604033a-9f7a-42a1-996b-52413bb10a71",
   "metadata": {},
   "source": [
    "### "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0c04d4a-e6b5-46a1-bba9-75582bf7e467",
   "metadata": {},
   "source": [
    "### "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1f8a571-d620-4d29-a99a-55ccafd65166",
   "metadata": {},
   "source": [
    "### "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff193d40-1389-4e68-b750-7c9db9e35c61",
   "metadata": {},
   "source": [
    "### "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99b4fd3c-5436-4e79-adab-42b1a5e2d5d0",
   "metadata": {},
   "source": [
    "### "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
